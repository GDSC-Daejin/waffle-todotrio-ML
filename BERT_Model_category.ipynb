{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hobby_data.txt\",'r', encoding='utf-8') as f :\n",
    "    hobby_data = f.readlines()\n",
    "hobby_data = [line.strip() for line in hobby_data]\n",
    "\n",
    "with open(\"data/housework_data.txt\",'r', encoding='utf-8') as f :\n",
    "    housework_data = f.readlines()\n",
    "housework_data = [line.strip() for line in housework_data]\n",
    "\n",
    "with open(\"data/meeting_data.txt\",'r', encoding='utf-8') as f :\n",
    "    meeting_data = f.readlines()\n",
    "meeting_data = [line.strip() for line in meeting_data]\n",
    "\n",
    "with open(\"data/study_data.txt\",'r', encoding='utf-8') as f :\n",
    "    study_data = f.readlines()\n",
    "study_data = [line.strip() for line in study_data]\n",
    "\n",
    "hobby_size = len(hobby_data)\n",
    "housework_size = len(housework_data)\n",
    "meeting_size = len(meeting_data)\n",
    "study_size = len(study_data)\n",
    "\n",
    "hobby = ['취미' for __ in range(hobby_size)]\n",
    "housework = ['가사' for __ in range(housework_size)]\n",
    "meeting = ['모임' for __ in range(meeting_size)]\n",
    "study = ['공부' for __ in range(study_size)]\n",
    "\n",
    "hobby_labeled = [(a,b) for a,b in zip(hobby_data, hobby)]\n",
    "housework_labeled = [(a,b) for a,b in zip(housework_data, housework)]\n",
    "meeting_labeled = [(a,b) for a,b in zip(meeting_data, meeting)]\n",
    "study_labeled = [(a,b) for a,b in zip(study_data, study)]\n",
    "\n",
    "labeled_data = hobby_labeled + housework_labeled + meeting_labeled + study_labeled\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "\n",
    "for d in labeled_data :\n",
    "    data.append(d[0])\n",
    "    label.append(d[1])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text' : data,\n",
    "                  'label' : label})\n",
    "\n",
    "label_map = {'취미': 0, '가사': 1, '모임': 2, '공부': 3}\n",
    "df['label'] = df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wasd3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "\n",
    "#토크나이저\n",
    "def Bert_Tokenizer(text_list, maximum_length = 16) :\n",
    "    attention_masks = []\n",
    "    input_ids = []\n",
    "\n",
    "    for text in text_list :\n",
    "        encoded = tokenizer.encode_plus(text, \n",
    "                                add_special_tokens=True ,\n",
    "                                max_length=maximum_length,\n",
    "                                truncation=True,\n",
    "                                padding='max_length',\n",
    "                                return_tensors='pt'\n",
    "                                )\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "#데이터셋 생성\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self, input_ids, attention_mask, label) :\n",
    "        self.input_ids = torch.cat(input_ids, dim=0)\n",
    "        self.attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        self.label = torch.tensor(label.values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return {\"input_ids\" : self.input_ids[idx],\n",
    "                \"attention_mask\" : self.attention_mask[idx],\n",
    "                \"label\" : self.label[idx]}\n",
    "\n",
    "\n",
    "#모델\n",
    "class BertClassifier(nn.Module) :\n",
    "    def __init__(self, bert_model, hidden_size, classes) :\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = bert_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classes = classes\n",
    "\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask) :\n",
    "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        cls = output.pooler_output\n",
    "        \n",
    "        x = self.fc1(cls)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "bert_model = BertModel.from_pretrained('kykim/bert-kor-base')\n",
    "model = BertClassifier(bert_model, 64, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "val_size = int(len(df) * 0.2)\n",
    "\n",
    "train_text = df['text'][val_size:]\n",
    "val_text = df['text'][:val_size]\n",
    "\n",
    "train_label = df['label'][val_size:]\n",
    "val_label = df['label'][:val_size]\n",
    "\n",
    "train_input_ids, train_attention_mask = Bert_Tokenizer(train_text)\n",
    "val_input_ids, val_attention_mask = Bert_Tokenizer(val_text)\n",
    "\n",
    "train_dataset = CustomDataset(train_input_ids, train_attention_mask, train_label)\n",
    "val_dataset = CustomDataset(val_input_ids, val_attention_mask, val_label)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.00001) # lr설정 매우 중요!\n",
    "\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3) :\n",
    "    for batch in train_dataloader :\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_f(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entrophy loss : 0.1076\n",
      "accuracy : 0.9697\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad() :\n",
    "    for batch in val_dataloader :\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_f(output, label)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "\n",
    "        correct += (preds==label).sum().item()\n",
    "        total_samples += label.size(0)\n",
    "        \n",
    "print(f\"cross entrophy loss : {total_loss/len(val_dataloader):.4f}\")\n",
    "print(f\"accuracy : {correct/total_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 외우기 : 공부\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasd3\\AppData\\Local\\Temp\\ipykernel_19632\\716302812.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_label = F.softmax(output).argmax().item()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "input_text = \"영어 단어 외우기\"\n",
    "input = [f\"{input_text}\"]\n",
    "\n",
    "input_input_ids, input_attention_mask = Bert_Tokenizer(input)\n",
    "\n",
    "with torch.no_grad() :\n",
    "    output = model(input_input_ids[0], input_attention_mask[0])\n",
    "    pred_label = F.softmax(output).argmax().item()\n",
    "\n",
    "reverse_label_map = {0 : '취미', 1 : '가사', 2 : '모임', 3 : '공부'}\n",
    "\n",
    "category = reverse_label_map[pred_label]\n",
    "\n",
    "print(f\"{input_text} : {category}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 후 저장\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
