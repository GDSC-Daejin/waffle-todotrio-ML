{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osl93\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\osl93\\AppData\\Local\\Temp\\ipykernel_15828\\1585104110.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_weights.pth\"))\n"
     ]
    }
   ],
   "source": [
    "#카테고리 분류\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from BERT import BertClassifier, Bert_Tokenizer\n",
    "    \n",
    "bert = BertModel.from_pretrained('kykim/bert-kor-base')\n",
    "model = BertClassifier(bert, 64, 4)\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "model.eval()\n",
    "\n",
    "def out_category(task) :\n",
    "    input = [f\"{task}\"]\n",
    "\n",
    "    input_ids, attention_mask = Bert_Tokenizer(input)\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        output = model(input_ids[0], attention_mask[0])\n",
    "        pred_label = F.softmax(output).argmax().item()\n",
    "\n",
    "    reverse_label_map = {0 : '취미', 1 : '가사', 2 : '모임', 3 : '공부'}\n",
    "\n",
    "    category = reverse_label_map[pred_label]\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osl93\\AppData\\Local\\Temp\\ipykernel_15828\\3233919529.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(samples.astype(int))\n",
      "C:\\Users\\osl93\\AppData\\Local\\Temp\\ipykernel_15828\\3233919529.py:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(np.random.choice([0, 1], size=size, p=[1 - prob, prob]))\n",
      "c:\\Users\\osl93\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#성공확률 예측\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_normal_hours(size=1, mean=336, std=50, min_val=24, max_val=8760):\n",
    "    samples = np.random.normal(mean, std, size)\n",
    "    samples = np.clip(samples, min_val, max_val)\n",
    "    return int(samples.astype(int))\n",
    "\n",
    "def generate_biased_labels(size=1, prob=0.9):\n",
    "    return int(np.random.choice([0, 1], size=size, p=[1 - prob, prob]))\n",
    "\n",
    "def mean(list) :\n",
    "    return sum(list) / len(list)\n",
    "\n",
    "n_sample = 500\n",
    "\n",
    "hours = []\n",
    "labels = []\n",
    "\n",
    "for __ in range(n_sample) :\n",
    "    hour = generate_normal_hours()\n",
    "    label = generate_biased_labels()\n",
    "    hours.append(hour)\n",
    "    labels.append(label)\n",
    "\n",
    "pri = ['HIGH', 'MEDIUM', 'LOW']\n",
    "pris = []\n",
    "\n",
    "for __ in range(n_sample) :\n",
    "    n = random.randint(0,2)\n",
    "    pris.append(pri[n])\n",
    "\n",
    "cat = ['가사', '취미', '공부', '모임']\n",
    "cats = []\n",
    "\n",
    "for __ in range(n_sample) :\n",
    "    n = random.randint(0,3)\n",
    "    cats.append(cat[n])\n",
    "\n",
    "df = pd.DataFrame({'hours' : hours,\n",
    "              'success' : labels})\n",
    "\n",
    "categories = pd.get_dummies(cats).astype(int)\n",
    "priorities = pd.get_dummies(pris).astype(int)\n",
    "\n",
    "df = pd.concat([categories, priorities, df], axis = 1)\n",
    "\n",
    "X_train = df.drop(columns=['success'])\n",
    "y_train = df['success']\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "def get_hours(createdDate, deadline) :\n",
    "    c = datetime.strptime(createdDate[:-7], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    d = datetime.strptime(deadline[:-4], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    t = d-c\n",
    "    return round(t.days*24 + t.seconds/3600, 1)\n",
    "\n",
    "def add_task(category, priority, createdDate, deadline, status):\n",
    "    new_row = {'가사': 0, '공부': 0, '모임': 0, '취미': 0, 'HIGH': 0, 'LOW': 0, 'MEDIUM': 0, 'hours': get_hours(createdDate, deadline), 'success': None}\n",
    "    new_row[category] = 1\n",
    "    new_row[priority] = 1\n",
    "    if status == 'COMPLETED' :\n",
    "        new_row['success'] = 1\n",
    "    elif status == 'DELAYED' :\n",
    "        new_row['success'] = 0\n",
    "    else :\n",
    "        new_row['success'] = None\n",
    "\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://192.168.219.113:5001\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "#서버실행\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/category', methods=['POST'])\n",
    "def predict_category():\n",
    "    data = request.json\n",
    "    task = data['content']\n",
    "    category = out_category(task)\n",
    "\n",
    "    return jsonify({\"category\": category})\n",
    "\n",
    "@app.route('/train', methods=['POST'])\n",
    "def model_train():\n",
    "    data = request.json \n",
    "    try :\n",
    "        task = add_task(data['category'], data['priority'], data['createdDate'], data['deadline'], data['status'])\n",
    "        train_data = pd.DataFrame([task])\n",
    "\n",
    "        global X_train\n",
    "        global y_train\n",
    "\n",
    "        X_train = pd.concat([X_train, train_data.drop(columns=['success'])], axis=0)\n",
    "        y_train = pd.concat([y_train, train_data['success']], axis=0)\n",
    "\n",
    "        logistic.fit(X_train, y_train)\n",
    "\n",
    "        model_result = \"Model trained successfully\"\n",
    "\n",
    "        return jsonify({'current_data': task, 'model':model_result})\n",
    "    except :\n",
    "        return jsonify({'error': 'Invalid data format'})\n",
    "\n",
    "@app.route('/prob', methods=['POST'])\n",
    "def receive_prob():\n",
    "    data = request.json\n",
    "\n",
    "    task = add_task(data['category'], data['priority'], data['createdDate'], data['deadline'], data['status'])\n",
    "    X_data = pd.DataFrame([task]).drop(columns=['success'])\n",
    "    \n",
    "    prob = (f'{logistic.predict_proba(X_data)[0][1] * 100:.0f}%')\n",
    "\n",
    "    return jsonify({'probability':prob})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port =5001, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
